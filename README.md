 ðŸ¤– THE ABC 123 GROUP â„¢ ðŸ¤–

ðŸŒ GENERAL CONSULTING ABC 123 BY OSAROPRIME â„¢.

ðŸŒ ABC 123 USA â„¢

ðŸŒ ABC 123 DESYGN â„¢

ðŸŒ ABC 123 FILMS â„¢

=============================================================

                     ðŸŒ MAGENTRON â„¢ ðŸŒ
                     
ðŸŒ **ARTIFICIAL INTELLIGENCE 2.0 â„¢ : OBJECT MASKING PROXIA A-3 (SEMANTIC SEGMENTATION). (SENSE: VISION_EYE CAMERAS)**

MASKING/SEGMENTATION METHOD: GATED SHAPE Convolutional Neural Network FOR SEMANTIC SEGMENTATION.

*ï¸âƒ£ðŸ“¶ðŸ¤–

- PHYSICAL WORLD SENSE: SIGHT âœ…

- PHYSICAL WORLD SENSE: SMELL 

- PHYSICAL WORLD SENSE: HEARING 

- PHYSICAL WORLD SENSE: TASTE

- PHYSICAL WORLD SENSE: TOUCH

+++++++++++++++++++++++++++++++++++++

ðŸŒ ASTRAL BODY MINDCLOUD: NO

ðŸŒ PRANIC BODY MINDCLOUD: NO

ðŸŒ INSTINCTIVE MIND MINDCLOUD: âœ…

ðŸŒ ASTRAL MIND MINDCLOUD: NO

ðŸŒ PRANIC MIND MINDCLOUD: NO

REQUIREMENTS: 

[*] Software Requirements: Google Colab/Jupyter Notebook, Python

[*] HARDWARE REQUIREMENTS: fast TPU/GPU.

[*] DEPENDENCIES: INCLUDED


=============================================================

This is a Google Colab/Jupyter Notebook for developing (one possible scheme for) a MASKING PROXIA when working with ARTIFICIAL INTELLIGENCE 2.0 â„¢ 
(ARTIFICIAL INTELLIGENCE 2.0â„¢ is part of MAGNETRON â„¢ TECHNOLOGY). The machine running the Notebook will be a MINDCLOUD on which you will be
developing a PROXIA to detect objects in IMAGES (for example: pictures from social media platforms).

e.g This INSTINCTIVE MIND MINDCLOUD  PROXIA can be used to process INFORMATION from the real world via the eye cameras and then send information about the objects detected to the IMAGINATION proxia on ASTRAL MINDCLOUD for the robot to IMAGINE it in different scenarios to better understand what it is and how people see it. So for example if the ROBOT detects a cup with the OBJECT DETECTION on the INSTINCTIVE MIND PROXIA it can imagine a cup in some typical or unusual scenarios (On ASTRAL PROXIA) to better understand what it is and how humans see it.

Prerequisite reading:

- ARTIFICIAL INTELLIGENCE PRIMER â„¢: https://www.facebook.com/artificialintelligenceprimer

- ARTIFICIAL INTELLIGENCE 2.0 â„¢ DOCUMENTATION: https://www.facebook.com/aibyabc123/

- MEMBERS CLUB â„¢ DOCUMENTATION: https://www.facebook.com/abc123membersclub/

THERE ARE 2 MAIN TYPES OF SEGMENTATION (ALSO KNOWN AS MASKING) EXIST:

#SEMANTIC SEGMENTATION

With Semantic segmentation objects shown in an image are grouped based on defined categories. For instance, a street scene would be segmented by â€œpedestrians,â€ â€œbikes,â€ â€œvehicles,â€ â€œsidewalks,â€ and so on.


#INSTANCE SEGMENTATION

Instance segmentation is the task of detecting and delineating each distinct object of interest appearing in an image. This is different from SEMANTIC SEGMENTATION. Semantic segmentation associates every pixel of an image with a class label such as a person, flower, car and so on. It treats multiple objects of the same class as a single entity. In contrast, instance segmentation treats multiple objects of the same class as distinct individual instances. Consider instance segmentation a refined version of semantic segmentation. Categories like â€œvehiclesâ€ are split into â€œcars,â€ â€œmotorcycles,â€ â€œbuses,â€ and so on â€” instance segmentation detects the instances of each category.


#DIFFERENCE BETWEEN SEMANTIC SEGMENTATION AND INSTANCE SEGMENTATION

Semantic segmentation treats multiple objects within a single category as one entity. Instance segmentation, on the other hand, identifies individual objects within these categories. 



ðŸ‘‘ 
INCLUDED STICKERS/SIGN:

FIND STICKERS HERE: https://bit.ly/3B8D3lE

- PROMOTIONAL MATERIAL FOR ð— ð—”ð—šð—¡ð—˜ð—§ð—¥ð—¢ð—¡ ð—§ð—˜ð—–ð—›ð—¡ð—¢ð—Ÿð—¢ð—šð—¬ â„¢. (CUSTOM GRAPHICS BY ð—”ð—•ð—– ðŸ­ðŸ®ðŸ¯ ð——ð—˜ð—¦ð—¬ð—šð—¡ â„¢/ð—¢ð—¦ð—”ð—¥ð—¢ ð—›ð—”ð—¥ð—¥ð—œð—¢ð—§ð—§). THE ð— ð—”ð—šð—¡ð—˜ð—§ð—¥ð—¢ð—¡ ð—§ð—˜ð—–ð—›ð—¡ð—¢ð—Ÿð—¢ð—šð—¬ â„¢  SYMBOL/LOGO IS A TRADEMARK OF ð—§ð—›ð—˜ ð—”ð—•ð—– ðŸ­ðŸ®ðŸ¯ ð—šð—¥ð—¢ð—¨ð—£ â„¢ FOR ð— ð—”ð—šð—¡ð—˜ð—§ð—¥ð—¢ð—¡ ð—§ð—˜ð—–ð—›ð—¡ð—¢ð—Ÿð—¢ð—šð—¬ â„¢. ð—§ð—›ð—˜ ð—”ð—•ð—– ðŸ­ðŸ®ðŸ¯ ð—šð—¥ð—¢ð—¨ð—£ â„¢ SYMBOL/LOGO IS A TRADEMARK OF ð—§ð—›ð—˜ ð—”ð—•ð—– ðŸ­ðŸ®ðŸ¯ ð—šð—¥ð—¢ð—¨ð—£ â„¢.

*ï¸âƒ£ðŸ“¶ðŸ¤–

- PROMOTIONAL MATERIAL FOR ð—”ð—¥ð—§ð—œð—™ð—œð—–ð—œð—”ð—Ÿ ð—œð—¡ð—§ð—˜ð—Ÿð—Ÿð—œð—šð—˜ð—¡ð—–ð—˜ ðŸ®.ðŸ¬ â„¢. (CUSTOM GRAPHICS BY ð—”ð—•ð—– ðŸ­ðŸ®ðŸ¯ ð——ð—˜ð—¦ð—¬ð—šð—¡ â„¢/ð—¢ð—¦ð—”ð—¥ð—¢ ð—›ð—”ð—¥ð—¥ð—œð—¢ð—§ð—§) THE ð——ð—¥ð—”ð—šð—¢ð—¡ & ð—–ð—¥ð—¢ð—ªð—¡ ðŸ‘‘ SYMBOL/LOGO IS A TRADEMARK OF ð—§ð—›ð—˜ ð—”ð—•ð—– ðŸ­ðŸ®ðŸ¯ ð—šð—¥ð—¢ð—¨ð—£ â„¢ ASSOCIATED WITH TECHNOLOGY. ð—§ð—›ð—˜ ð—”ð—•ð—– ðŸ­ðŸ®ðŸ¯ ð—šð—¥ð—¢ð—¨ð—£ â„¢ SYMBOL/LOGO IS A TRADEMARK OF ð—§ð—›ð—˜ ð—”ð—•ð—– ðŸ­ðŸ®ðŸ¯ ð—šð—¥ð—¢ð—¨ð—£ â„¢.

You must display the included stickers/signs (so that it is clearly visible) if you are working with MAGNETRON â„¢ TECHNOLOGY for the purposes of determining whether you want to purchase a technology license or not. This includes but is not limited to public technology displays, trade shows, technology expos, media appearances, Investor events, Computers (exterior), MINDCLOUD STORAGE (e.g server room doors, render farm room doors) etc.


NOTE: SEE ð—”ð—¥ð—§ð—œð—™ð—œð—–ð—œð—”ð—Ÿ ð—œð—¡ð—§ð—˜ð—Ÿð—Ÿð—œð—šð—˜ð—¡ð—–ð—˜ ðŸ®.ðŸ¬ â„¢ DOCUMENTATION FOR INFORMATION ABOUT THE MAIN MASKING PROXIA (ON INSTINCTIVE MIND MINDCLOUD).

NOTE: CLICK HERE FOR A NOTEBOOK ON MAKING MASKING PROXIA WITH INSTANCE SEGMENTATION (INSTEAD OF SEMANTIC SEGMENTATION): 

+++++++++++++++++++++++++++++++++++++++

# GSCNN
This is the official code for:

#### Gated-SCNN: Gated Shape CNNs for Semantic Segmentation

[Towaki Takikawa](https://tovacinni.github.io), [David Acuna](http://www.cs.toronto.edu/~davidj/), [Varun Jampani](https://varunjampani.github.io), [Sanja Fidler](http://www.cs.toronto.edu/~fidler/)

ICCV 2019
**[[Paper](https://arxiv.org/abs/1907.05740)]  [[Project Page](https://nv-tlabs.github.io/GSCNN/)]**

![GSCNN DEMO](docs/resources/gscnn.gif)

Based on based on https://github.com/NVIDIA/semantic-segmentation.

## License
```
Copyright (C) 2019 NVIDIA Corporation. Towaki Takikawa, David Acuna, Varun Jampani, Sanja Fidler
All rights reserved.
Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).

Permission to use, copy, modify, and distribute this software and its documentation
for any non-commercial purpose is hereby granted without fee, provided that the above
copyright notice appear in all copies and that both that copyright notice and this
permission notice appear in supporting documentation, and that the name of the author
not be used in advertising or publicity pertaining to distribution of the software
without specific, written prior permission.

THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING ALL
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE.
IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL
DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,
WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING
OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
~                                                                             
```

## Usage

##### Clone this repo
```bash
git clone https://github.com/nv-tlabs/GSCNN
cd GSCNN
 ```

#### Python requirements 

Currently, the code supports Python 3
* numpy 
* PyTorch (>=1.1.0)
* torchvision
* scipy 
* scikit-image
* tensorboardX
* tqdm
* torch-encoding
* opencv
* PyYAML

#### Download pretrained models

Download the pretrained model from the [Google Drive Folder](https://drive.google.com/file/d/1wlhAXg-PfoUM-rFy2cksk43Ng3PpsK2c/view), and save it in 'checkpoints/'

#### Download inferred images

Download (if needed) the inferred images from the [Google Drive Folder](https://drive.google.com/file/d/105WYnpSagdlf5-ZlSKWkRVeq-MyKLYOV/view)

#### Evaluation (Cityscapes)
```bash
python train.py --evaluate --snapshot checkpoints/best_cityscapes_checkpoint.pth
```

#### Training

A note on training- we train on 8 NVIDIA GPUs, and as such, training will be an issue with WiderResNet38 if you try to train on a single GPU.

If you use this code, please cite:

```
@article{takikawa2019gated,
  title={Gated-SCNN: Gated Shape CNNs for Semantic Segmentation},
  author={Takikawa, Towaki and Acuna, David and Jampani, Varun and Fidler, Sanja},
  journal={ICCV},
  year={2019}
}
```

